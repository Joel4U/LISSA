# LISSA: Linguistically-Informed Span Self-Attention

> **Small Language Model Makes an Effective Long Text Extractor**

[![arXiv](https://img.shields.io/badge/arXiv-2502.07286-b31b1b.svg)](https://arxiv.org/html/2502.07286v1#bib.bib43)

## üìñ Abstract

This repository contains the implementation of **LISSA (Linguistically-Informed Span Self-Attention)**, a novel approach that demonstrates how small language models can effectively extract information from long texts through linguistically-informed span-level attention mechanisms.

**Paper**: [Small Language Model Makes an Effective Long Text Extractor](https://arxiv.org/html/2502.07286v1#bib.bib43)

## üöÄ Key Features

- **Small Model, Big Performance**: Efficient long text processing with reduced computational overhead
- **Linguistically-Informed**: Incorporates syntactic dependency knowledge for better understanding
- **Span-Level Attention**: Novel attention mechanism operating at span level for improved extraction
- **Long Text Capability**: Handles documents significantly longer than typical model limits

## üìä Results

Our approach achieves state-of-the-art performance on long text extraction tasks while using significantly fewer parameters than large language models.

## üõ†Ô∏è Installation

```bash
git clone https://github.com/yourusername/LISSA
cd LISSA
pip install -r requirements.txt
